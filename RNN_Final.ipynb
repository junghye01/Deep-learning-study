{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3K43geEgck5Njge2az+xi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["자연어 전처리"],"metadata":{"id":"tfsiMys3qDYv"}},{"cell_type":"code","source":["!pip uninstall torchtext\n","!pip install torchtext==0.4.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"IpKZvr8BSH2L","executionInfo":{"status":"ok","timestamp":1677103192457,"user_tz":-540,"elapsed":11860,"user":{"displayName":"김정혜","userId":"12576703694961924608"}},"outputId":"5adc9e94-9bf1-443e-8e68-045d7f464709"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torchtext 0.4.0\n","Uninstalling torchtext-0.4.0:\n","  Would remove:\n","    /usr/local/lib/python3.8/dist-packages/torchtext-0.4.0.dist-info/*\n","    /usr/local/lib/python3.8/dist-packages/torchtext/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled torchtext-0.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.4.0\n","  Using cached torchtext-0.4.0-py3-none-any.whl (53 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.4.0) (4.64.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from torchtext==0.4.0) (1.15.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchtext==0.4.0) (1.13.1+cu116)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.4.0) (2.25.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.4.0) (1.22.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.4.0) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.4.0) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.4.0) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.4.0) (4.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchtext==0.4.0) (4.5.0)\n","Installing collected packages: torchtext\n","Successfully installed torchtext-0.4.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torchtext"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oagpnL1F1xKs","executionInfo":{"status":"ok","timestamp":1677103210644,"user_tz":-540,"elapsed":2202,"user":{"displayName":"김정혜","userId":"12576703694961924608"}},"outputId":"4bd1640d-b34e-48c1-de14-e6bf1636d023"},"outputs":[{"output_type":"stream","name":"stdout","text":["다음 기기로 학습합니다: cuda\n"]}],"source":["import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torchtext import data,datasets\n","\n","\n","# 하이퍼파라미터\n","BATCH_SIZE = 64\n","lr = 0.001\n","EPOCHS = 10\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","print(\"다음 기기로 학습합니다:\", DEVICE)\n","\n"]},{"cell_type":"code","source":["#전처리 정의\n","TEXT=data.Field(sequential=True,batch_first=True,lower=True) # sequential : 순차적이면 True , batch_first : True면 신경망에 입력되는 텐서의 첫 번째 차원값이 batch_size\n","LABEL=data.Field(sequential=False,batch_first=True) \n","\n","#학습셋과 테스트셋\n","trainset,testset=datasets.IMDB.splits(TEXT,LABEL) # TabularDataset : text와 label을 속성으로 가짐\n","\n","#워드 임베딩에 필요한 단어 사전\n","TEXT.build_vocab(trainset,min_freq=5) # 최소 5번 이상 등장한 단어만 사전에 담겠다는 것  # buld_vocab : 정수 인코딩 작업 전에 단어들의 집합 생성\n","LABEL.build_vocab(trainset)"],"metadata":{"id":"KOwKDamhAwXw","executionInfo":{"status":"ok","timestamp":1677103225799,"user_tz":-540,"elapsed":12174,"user":{"displayName":"김정혜","userId":"12576703694961924608"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# valset (훈련셋의 20%를 검증셋으로)\n","trainset,valset=trainset.split(split_ratio=0.8)\n","train_iter,val_iter,test_iter=data.BucketIterator.splits((trainset,valset,testset),batch_size=BATCH_SIZE,shuffle=True,repeat=False)\n"],"metadata":{"id":"s6ojDMLNSuxt","executionInfo":{"status":"ok","timestamp":1677103230404,"user_tz":-540,"elapsed":378,"user":{"displayName":"김정혜","userId":"12576703694961924608"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["vocab_size=len(TEXT.vocab) # 사전 속 단어들의 개수\n","n_classes=2\n"],"metadata":{"id":"I0FUjcD02ygJ","executionInfo":{"status":"ok","timestamp":1677103265130,"user_tz":-540,"elapsed":419,"user":{"displayName":"김정혜","userId":"12576703694961924608"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print('[학습셋]: %d [검증셋] :%d [테스트셋]:%d [단어수]:%d [클래스]:%d'%(len(trainset),len(valset),len(testset),vocab_size,n_classes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRwinLvGv8oF","executionInfo":{"status":"ok","timestamp":1677103267446,"user_tz":-540,"elapsed":364,"user":{"displayName":"김정혜","userId":"12576703694961924608"}},"outputId":"074cbd51-a5af-4a65-8f18-fb2c634563db"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[학습셋]: 20000 [검증셋] :5000 [테스트셋]:25000 [단어수]:46159 [클래스]:2\n"]}]},{"cell_type":"code","source":["#trainset 구성(TabularDataset)\n","print(trainset[0].text,trainset[0].label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKd7XJErQ5vb","executionInfo":{"status":"ok","timestamp":1677103269965,"user_tz":-540,"elapsed":373,"user":{"displayName":"김정혜","userId":"12576703694961924608"}},"outputId":"0ac4b22f-3d2d-4c56-ff1f-3759cdace391"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['first', 'off', 'there', 'is', 'nothing', 'wrong', 'with', 'studying', 'daniel,', 'revelation,', 'matthew', '24,', 'isaiah', 'and', 'other', 'prophetic', 'scriptures.', 'there', 'is', 'also', 'nothing', 'wrong', 'with', 'making', 'a', 'film', 'such', 'as', 'this', 'to', 'attempt', 'to', 'present', 'the', 'gospel', 'message.', 'so', 'my', 'qualms', 'with', 'this', 'movie', 'are', 'not', 'in', 'either', 'its', 'sincerity', 'or', 'aspirations.', 'as', 'a', 'christian,', 'though', 'an', 'amillenialist,', 'i', 'believe', 'there', 'will', 'be', 'a', 'great', 'tribulation', 'and', 'i', 'believe', 'christ', 'will', 'return', 'as', 'he', 'said', 'as', 'much.', 'so', 'even', 'though', 'i', 'have', 'disagreement', 'with', 'this', 'film', 'about', 'the', 'rapture', 'that', 'is', 'not', 'why', 'i', 'rate', 'this', 'movie', 'so', 'low.<br', '/><br', '/>no,', 'what', 'makes', 'me', 'rate', 'this', 'movie', 'so', 'low', 'is', 'not', 'its', 'sincerity', 'or', 'its', 'message,', 'but', 'rather', 'its', 'lack', 'of', 'production', 'values,', 'awful', 'script,', 'mediocre', 'acting,', 'and', 'pitiful', 'fx.', 'this', 'movie', 'ranks', 'down', 'there', 'with', 'some', 'of', 'the', 'cheesiest', 'scifi', 'fodder', 'of', 'the', '1950s.', 'no,', 'this', 'movie', 'ranks', 'down', 'there', 'with', 'plan', '9', 'from', 'outerspace.', 'this', 'movie', 'failed', 'to', 'age', 'well', 'and', 'was', 'probably', 'dated', 'by', 'the', 'time', 'they', 'made', 'a', 'sequel.<br', '/><br', '/>the', 'apocalypse', 'genre', 'film', 'producers', 'could', 'have', 'learned', 'how', 'not', 'to', 'make', 'an', 'end', 'times', 'film', 'from', 'this,', 'but', 'they', 'failed.', 'the', 'left', 'behind', 'series,', 'the', 'apocalypse', 'series,', 'and', 'the', 'omega', 'code', 'series', 'all', 'failed', 'to', 'learn', 'from', 'this', 'because', 'they', 'addressed', 'the', 'fx', 'problems', 'and', 'the', 'dated', 'look', 'problem,', 'but', 'their', 'scripts', 'are', 'still', 'poor,', 'and', 'their', 'acting', 'is', 'wooden.<br', '/><br', '/>there', 'are', 'great', 'christian', 'films,', 'with', 'extremely', 'low', 'budgets,', 'but', 'this', 'film', 'is', 'not', 'one', 'of', 'them.', \"i'm\", 'surprised', 'the', 'mst3k', 'crew', 'never', 'lampooned', 'this', 'one.'] neg\n"]}]},{"cell_type":"markdown","source":["RNN 모델 구현"],"metadata":{"id":"mWPMljwuyabg"}},{"cell_type":"code","source":["class BasicGRU(nn.Module):\n","  def __init__(self,n_layers,hidden_dim,n_vocab,embed_dim,n_classes,dropout_p=0.2):\n","    super(BasicGRU,self).__init__()\n","    print('Buliding Basic GRU model...')\n","\n","    self.n_layers=n_layers\n","    self.embed=nn.Embedding(n_vocab,embed_dim) # 단어 집합 개수만큼 임베딩 벡터를 가진 임베딩 테이블 (자연어 -> 임베딩 벡터로 맵핑)\n","\n","    self.hidden_dim=hidden_dim # 은닉벡터의 차원값\n","    self.dropout=nn.Dropout(dropout_p)\n","\n","    self.gru=nn.GRU(embed_dim,self.hidden_dim,num_layers=self.n_layers,batch_first=True) # RNN : 입력이 길어지면 학습도중 기울기가 너무 커지거나 작아지는 현상이 발생, gru : rnn의 단점을 보완하여 기울기를 적정하게 유지하고 문장 앞부분의 정보가 뒷부분에도 전달될 수 있게 함\n","    self.out=nn.Linear(self.hidden_dim,n_classes)\n","\n","  def forward(self,x):\n","        x=self.embed(x)\n","        h_0=self._init_state(batch_size=x.size(0)) # 첫 번째 은닉벡터, RNN 신경망은 입력과 첫번째 은닉벡터를 입력해줘야함 \n","        x, _ = self.gru(x, h_0)  # [i, b, h]\n","        h_t = x[:,-1,:] # 모든 리뷰들을 압축한 은닉벡터 \n","        self.dropout(h_t)\n","        logit = self.out(h_t)  # [b, h] -> [b, o] #\n","        return logit # 결과 출력 \n","\n","  def _init_state(self,batch_size=1):\n","      weight=next(self.parameters()).data\n","      return weight.new(self.n_layers,batch_size,self.hidden_dim).zero_()\n","    \n","\n","\n"],"metadata":{"id":"Fp6CEtaAyZc_","executionInfo":{"status":"ok","timestamp":1677103305581,"user_tz":-540,"elapsed":618,"user":{"displayName":"김정혜","userId":"12576703694961924608"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train(model,optimizer,train_iter):\n","    model.train()\n","    for b,batch in enumerate(train_iter): # 반복마다 배치데이터 반환\n","        x,y=batch.text.to(DEVICE),batch.label.to(DEVICE) # text : 영화평 데이터, label : 레이블 \n","\n","        y.data.sub_(1) # label을 0,1로 바꿈\n","\n","        optimizer.zero_grad() # 기울기 0으로 초기화\n","        logit=model(x) # 예측값\n","\n","        loss=F.cross_entropy(logit,y) # 오차\n","        loss.backward()\n","        optimizer.step() # 최적화 \n","\n","\n","def evaluate(model,val_iter):\n","    model.eval()\n","    corrects, total_loss = 0, 0\n","    for batch in val_iter:\n","        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n","        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n","        logit = model(x)\n","        loss = F.cross_entropy(logit, y, reduction='sum')\n","        total_loss += loss.item()\n","        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n","    size = len(val_iter.dataset)\n","    avg_loss = total_loss / size\n","    avg_accuracy = 100.0 * corrects / size\n","    return avg_loss, avg_accuracy\n"],"metadata":{"id":"QNHMP9GgBKMF","executionInfo":{"status":"ok","timestamp":1677103307702,"user_tz":-540,"elapsed":5,"user":{"displayName":"김정혜","userId":"12576703694961924608"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#모델 객체 정의\n","model=BasicGRU(1,256,vocab_size,128,n_classes,0.5).to(DEVICE)  # 은닉벡터의 차원값 256, 임베딩된토큰의 차원값 128\n","optimizer=torch.optim.Adam(model.parameters(),lr=lr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfwT9G8_HLnE","executionInfo":{"status":"ok","timestamp":1677103309597,"user_tz":-540,"elapsed":3,"user":{"displayName":"김정혜","userId":"12576703694961924608"}},"outputId":"b9eba691-8769-4efc-a5fc-44cc928ca3c3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Buliding Basic GRU model...\n"]}]},{"cell_type":"code","source":["# 최종 모델 : 검증오차가 가장 작은 모델\n","best_val_loss = None\n","for e in range(1, EPOCHS+1):\n","    train(model, optimizer, train_iter)\n","    val_loss, val_accuracy = evaluate(model, val_iter)\n","\n","    print(\"[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (e, val_loss, val_accuracy))\n","    \n","    # 검증 오차가 가장 적은 최적의 모델을 저장\n","    if not best_val_loss or val_loss < best_val_loss:\n","        if not os.path.isdir(\"snapshot\"):\n","            os.makedirs(\"snapshot\")\n","        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n","        best_val_loss = val_loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCnywaD9St8u","executionInfo":{"status":"ok","timestamp":1677103552576,"user_tz":-540,"elapsed":239088,"user":{"displayName":"김정혜","userId":"12576703694961924608"}},"outputId":"0daa8e37-c141-4bd8-cae1-05a4f6d6b501"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[이폭: 1] 검증 오차: 0.70 | 검증 정확도:50.34\n","[이폭: 2] 검증 오차: 0.68 | 검증 정확도:58.40\n","[이폭: 3] 검증 오차: 0.44 | 검증 정확도:81.58\n","[이폭: 4] 검증 오차: 0.33 | 검증 정확도:87.00\n","[이폭: 5] 검증 오차: 0.32 | 검증 정확도:87.22\n","[이폭: 6] 검증 오차: 0.34 | 검증 정확도:86.84\n","[이폭: 7] 검증 오차: 0.35 | 검증 정확도:87.24\n","[이폭: 8] 검증 오차: 0.39 | 검증 정확도:86.62\n","[이폭: 9] 검증 오차: 0.41 | 검증 정확도:86.38\n","[이폭: 10] 검증 오차: 0.42 | 검증 정확도:86.70\n"]}]},{"cell_type":"code","source":["#테스트 셋으로 모델 성능 평가\n","\n","model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n","test_loss, test_acc = evaluate(model, test_iter)\n","print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2EgovLcQRAr","executionInfo":{"status":"ok","timestamp":1677103612949,"user_tz":-540,"elapsed":5008,"user":{"displayName":"김정혜","userId":"12576703694961924608"}},"outputId":"84e83c02-6a05-42d4-bf2a-a247a4a8e27e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["테스트 오차:  0.33 | 테스트 정확도: 86.62\n"]}]}]}